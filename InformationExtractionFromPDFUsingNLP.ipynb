{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Imports and Installs"
      ],
      "metadata": {
        "id": "VOLc7mJui-eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "!pip install PyPDF2\n",
        "!pip install bs4\n",
        "# !pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udtBjeeP096E",
        "outputId": "e22fd289-d1e0-4d5b-b81e-fe5b91c7127e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "import spacy\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import requests\n",
        "from bs4 import *\n",
        "import wikipedia\n",
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "id": "nN_IR9Fz6rOV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuSgBsCt7tJW",
        "outputId": "7a8923c9-1150-4911-c62e-d1711ca76d81"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inetntion and explanation: In the below code I have used some libraries that are not readily available in python and thus theyhave been nstalled and the others that are available have been imported."
      ],
      "metadata": {
        "id": "PFdm9HH2Bdq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword detector"
      ],
      "metadata": {
        "id": "Cgcf2A9-xFzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_named_entity_recognition(preprocessed_text):\n",
        "    doc = nlp(preprocessed_text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG','PERSON']:\n",
        "            entities.append(ent.text)\n",
        "    return entities\n",
        "\n",
        "\n",
        "def extract_keywords_tfidf(preprocessed_text):\n",
        "    # Tokenizing the text and removing the stop words\n",
        "    token = word_tokenize(preprocessed_text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_token = [words.lower() for words in token if words.lower() not in stop_words]\n",
        "\n",
        "    # Lemmatizing the newly created tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_token = [lemmatizer.lemmatize(words) for words in filtered_token]\n",
        "\n",
        "    # Creating a string from the tokens\n",
        "    processed_text = ' '.join(lemmatized_token)\n",
        "\n",
        "    #TF-IDF calculations\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([processed_text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Using the TF-IDF scores to sort features\n",
        "    sorted_indices = tfidf_matrix.toarray()[0].argsort()[::-1]\n",
        "    keywords = [feature_names[idx] for idx in sorted_indices]\n",
        "    return keywords\n",
        "\n",
        "def get_top_keywords_tfidf(keywords, n):\n",
        "    top_keywords = keywords[:n]\n",
        "    return top_keywords"
      ],
      "metadata": {
        "id": "cZGziZfMxQQn"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inetntion and explanation: Here I have used two information retrivel techniques one is NER(Named Entity Recognition) and the lother is TF-IDF. The reason for chosing these both was simple, ehile the NER will give me keywords that are more directed and categorized, TF-IDF will allow me to get more relevent words to the document text."
      ],
      "metadata": {
        "id": "70_T2geFCE-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "ZIouMWjPwh0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analysis(preprocessed_text):\n",
        "  sentiment_blob = TextBlob(preprocessed_text)\n",
        "  return sentiment_blob.sentiment"
      ],
      "metadata": {
        "id": "6-XG9tZmw71p"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inetntion and explanation: This will return Polarity and Subjectivty of the text hence allowing us to conclude how bias and skewed the text is."
      ],
      "metadata": {
        "id": "E67wMJpSET85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Clouds"
      ],
      "metadata": {
        "id": "JvY8v4AawS91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keywords_tfidf_visualize(keywords):\n",
        "    # Creating a string from the TFIDF keywords\n",
        "    text = ' '.join(keywords)\n",
        "\n",
        "    # Word cloud\n",
        "    wordcloud = WordCloud(background_color='#f0f0f0',\n",
        "                          width=1920,\n",
        "                          height=1080).generate(text)\n",
        "\n",
        "    # Plotting the Wordcloud\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(\" Word Cloud representing TF-IDF\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def keywords_ner_visualize(entities):\n",
        "    # Creating a string from the named entity recogniction keywords\n",
        "    text = ' '.join(entities)\n",
        "\n",
        "    # Word cloud\n",
        "    wordcloud = WordCloud(background_color='#f0f0f0',\n",
        "                          width=1920,\n",
        "                          height=1080).generate(text)\n",
        "\n",
        "    # Plotting the Wordcloud\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(\" Word Cloud representing Names entity recognition Keywords\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "PzAXyiXzwtlS"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intention and Explantion: USing this we will be creating a wordcloud as a form of representation for NER Keywords and TF-IDF Keywords"
      ],
      "metadata": {
        "id": "rhyVPso-MDDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meanings of Top Keywords"
      ],
      "metadata": {
        "id": "CH5eJa01wZXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_wikipedia_defination(top_keywords):\n",
        "\n",
        "    definitions = []\n",
        "    for keyword in top_keywords:\n",
        "        try:\n",
        "            wikepedia_definations = wikipedia.summary(keyword, sentences=3)\n",
        "            definitions.append(wikepedia_definations)\n",
        "\n",
        "        except wikipedia.exceptions.DisambiguationError as de:\n",
        "            #Disambiguation error\n",
        "            print(f\"DisambiguationError:'{keyword}' too ambigious by spacy\")\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            #Page not found error\n",
        "            print(f\"PageError: '{keyword}'  defination not found on Wikipedia\")\n",
        "    return definitions"
      ],
      "metadata": {
        "id": "wkcbU8HjVVSz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Detector from a PDF"
      ],
      "metadata": {
        "id": "81iX-ARvwIPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intention and Explantion: Below The code gets text from the pdf and then us ethe above methods to give us results and with that information we can make a decision on whether this pdf is for us or not,"
      ],
      "metadata": {
        "id": "HHIa8Qt2Oxfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def frequency_matrix(sentences, stop_words):\n",
        "    frequency_matrix = {}\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            if word in stop_words:\n",
        "                continue\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "    return frequency_matrix\n",
        "\n",
        "def tf_matrix(frequency_matrix):\n",
        "    tf_matrix = {}\n",
        "    for sent, f_table in frequency_matrix.items():\n",
        "        tf_table = {}\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "        tf_matrix[sent] = tf_table\n",
        "    return tf_matrix\n",
        "\n",
        "def sentence_score(tf_matrix):\n",
        "    sentence_scores = {}\n",
        "    for sent, tf_table in tf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "        for word, tf in tf_table.items():\n",
        "            total_score_per_sentence += tf\n",
        "        sentence_scores[sent] = total_score_per_sentence\n",
        "    return sentence_scores\n",
        "\n",
        "\n",
        "def generate_summary_for_text(preprocessed_texts, sent):\n",
        "    sentences = sent_tokenize(preprocessed_texts)\n",
        "    stop_words = stopwords.words('english')\n",
        "    frequency_matrix_output = frequency_matrix(sentences, stop_words)\n",
        "    tf_matrix_output = tf_matrix(frequency_matrix_output)\n",
        "    sentence_scores = sentence_score(tf_matrix_output)\n",
        "\n",
        "    ranked_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "    summary = '. '.join(ranked_sentences[:sent])\n",
        "\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "vWg_1rZ1-Kkz"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    preprocessed_text = ''\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            preprocessed_text += page.extract_text()\n",
        "    return preprocessed_text\n"
      ],
      "metadata": {
        "id": "otfChLoSA82B"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_detection_visualization_and_wikipedia_definations(file_path):\n",
        "    # Extracting the text from the PDF\n",
        "    preprocessed_texts = extract_text_from_pdf(file_path)\n",
        "    print(\"Below are all the results of all the keyword summary and visualization for the same attached to help undertsand the pdf\")\n",
        "\n",
        "    #  Named entity recognition (NER) Keywords\n",
        "    entities = perform_named_entity_recognition(preprocessed_texts)\n",
        "    print(\"Named entity Recognition Keywords\")\n",
        "    print(entities)\n",
        "\n",
        "\n",
        "    # TF-IDF Keywords\n",
        "    tfidf_keywords = extract_keywords_tfidf(preprocessed_texts)\n",
        "    top_keywords = get_top_keywords_tfidf(tfidf_keywords, 10)\n",
        "    print(\"TF-IDF Keywords\")\n",
        "    print(top_keywords)\n",
        "\n",
        "\n",
        "  # Wikipedia defination\n",
        "    defination = extract_wikipedia_defination(top_keywords)\n",
        "    print(defination)\n",
        "\n",
        "\n",
        "  # Generate summary\n",
        "    generated_summary = generate_summary_for_text(preprocessed_texts, 500)\n",
        "    print(generated_summary)\n",
        "\n",
        "\n",
        "   # WordCloud\n",
        "    keywords_tfidf_visualize(tfidf_keywords)\n",
        "    keywords_ner_visualize(entities)\n",
        "\n",
        "  # Sentiment Analysis of the pdf\n",
        "    sentiment = sentiment_analysis(preprocessed_texts)\n",
        "    print(sentiment)\n",
        "\n",
        "\n",
        "# Calling the program finally\n",
        "file_path = \"/content/riot-impactreport-2022.pdf\" # PDF path here\n",
        "keyword_detection_visualization_and_wikipedia_definations(file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aiVqhqrIG5v",
        "outputId": "35363470-2f9c-44c4-d993-5b2f1b2e1838"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below are all the results of all the keyword summary and visualization for the same attached to help undertsand the pdf\n",
            "Right to Know\n",
            "Y. These \n",
            "efforts . 1 - Purchased G. Rioters \n",
            "come f. Both Riot Noir,. It’s not the re. We’ve seen the . In the years si. In 2022 we open. IN IRELAND, Rio. With Covid rest. In the U.S., th. We’ll \n",
            "report o. We know that pr. This leads to h. Senna, Lucian, . And finally, Ri. In May, Riot fu. As we’ve grown . Topics covered . We’re thankful . To make sure ea. The Filipino ag. Players are the. In 2022 \n",
            "we wer. Behind the scen. Every time we d. Donation Matchi. While RIGs offe. Across League o. By \n",
            "supporting . These kits incl. 2022 marked the. Instead of bein. In areas hit by. Club Rainbow lo. Renata Glasc Ni. By working dire. As a global com. Take This\n",
            "In pa. Our grantees al. Riot teamed up . Fade’s design i. We’re committed. The center aims. Korean \n",
            "players. On behalf of th. We would not be. When Worlds tra. Someone who has. The surveys ask. Riot is just a . For example, th. ESA created a s. Riot’s \n",
            "MacBook. Funded in part . In 2022, we beg. The vast majori. RIGs organize c. Rioters in \n",
            "Bra. During the even. In China, Riote. When we provide. To help reinfor. Riot placed \n",
            "th. It means being . These champions. Every year more. My personal fav. By inspiring, c. Because of this. 6,907 tCO2e\n",
            "SCO. Now that we hav. 100% of that \n",
            "m. Our localizatio. Each year, the . Making sure the. Thanks to their. In addition, 4.. It means giving. Riot added anot. There’s no Blac. These are essen. Our \n",
            "partnershi. 14.6% of eligib. We recognized \n",
            ". In all, the cha. And it \n",
            "means i. We’ve \n",
            "opened u. This has been \n",
            ". → 60 students  . If you’ve read . Riot’s generous. Transparent wit. VALORANT’s Newe. 92,369 tCO2e\n",
            "TO. A \n",
            "large part o. Those four \n",
            "ski. If the index sc. This year’s gra. → Rioters raise. That’s why it’s. Increased ferti. But good repres. Without their \n",
            ". tCO2eThroughout. All of the work. We’re so \n",
            "fortu. In the U.S. the. Provided an ann. The goal of the. Each discussion. We \n",
            "initially c. Our partnership. Rioters went ab. The chem-barone. With that in mi. In Los Angeles,. Emissions inclu. SCOPE 1 EMISSIO. As we’ve expand. From a possesse. She’s also voic. Every player gl. Working directl. Of the \n",
            "$6.2M a. This \n",
            "inventory. 775K\n",
            "players vo. Inclusion Index. We’re also goin. From mentorship. Currently, we h. By working with. The average pas. After a decade-. The Game \n",
            "Chang. Riot Games is u. While our benef. Donating to int. Our total repor. Riot’s leadersh. Joan Ganz Coone. 2022\n",
            "ANNUAL\n",
            "IMP. In 2023, we’re . In 2022, Riot i. The opt-in resp. PEOPLE SUPPORT . That starts wit. LGBTQ+ \n",
            "Increas. Rest and Rejuve. I was so inspir. When we find th. This has \n",
            "led t. Outside of K’Sa. The result is a. $5MVALORANT Giv. Through the Giv. We want to \n",
            "res. This is a requi. In 2022 we spen. It means tappin. Evolving cultur. We went \n",
            "from o. We brought more. And those are j. Tracking these . The GRS is our . In addition, Ri. This process us. As a major \n",
            "gam. When we come to. The Riot Games . That’s how the . THROUGHOUT THE . Supporting play. This \n",
            "year Riot. In August 2022,. Legendary Suppo. He shared that . As we tell thes. He’s a tank who. And a fabled mo. They’re all def. No one is in ch. Co-\n",
            "designed by. Our global team. Charity Voting . But localizatio. Being close to . There are curre. Out of the game. The support we . Bots and inauth. Keeping Data On. It’s clear from. That being said. Unlike in VALOR. In 2022 we fulf. We know \n",
            "there . So what are we . In 2022, we par. Across our titl. By combining ou. Right now machi. Riot Games is t. The two-day sum. Here are the re. We’re still in . And we are luck. Riot as a compa. Games have the . When Riot start. Since 2019 our . We invested mor. Gaming ‘Cxmmuni. Riot’s employee. Since the Socia. One of those pr. For the last se. It was an overa. This year one o. This year we re. When a parent \n",
            ". If you tried to. Designing K’San. That’s not to s. And so we \n",
            "want. 28 regions were. They also creat. That may seem o. So if you want . It’s \n",
            "one of th. These automated. We want to do o. The center prov. In an effort \n",
            "t. This mantra inf. So to the peopl. As a company, w. While there hav. THE KICKBACKPEO. They were given. The Cultural \n",
            "H. 2022 saw a vari. A man who gives. I spent a lot o. For the most pa. In \n",
            "the Philipp. Localization is. Localization he. In 2022, we lau. In 2022 , we an. We are actively. Based on the re. Between monthly. The Social Impa. We activated in. Rioters shared . Showing the nex. In 2022, the te. In 2021, we \n",
            "la. In addition, hC. In the first ye. This challenge . The GHG Protoco. Our ability to . Because of Riot. After the four-. In 2022 we took. For the \n",
            "last f. They can \n",
            "also . 1,735 tCO2e\n",
            "SCO\n"
          ]
        }
      ]
    }
  ]
}